import sys
import os
import json
import pandas as pd
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# ThÃªm Ä‘Æ°á»ng dáº«n root Ä‘á»ƒ import Ä‘Æ°á»£c app
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# Import Graph cá»§a báº¡n
from app.services.law_agent.graph import app as agent_app
from app.core.config import settings

# 1. Cáº¥u hÃ¬nh Model cháº¥m Ä‘iá»ƒm (DÃ¹ng GPT-4o-mini hoáº·c GPT-3.5 cho ráº»)
# Ragas cáº§n Model riÃªng Ä‘á»ƒ lÃ m giÃ¡m kháº£o
evaluator_llm = ChatOpenAI(model="gpt-4o-mini", api_key=settings.OPENAI_API_KEY)
evaluator_embeddings = OpenAIEmbeddings(api_key=settings.OPENAI_API_KEY)

def run_evaluation():
    print("ğŸš€ Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh Evaluation...")
    
    # 2. Load dá»¯ liá»‡u test
    with open("tests/evaluation_dataset.json", "r", encoding="utf-8") as f:
        test_data = json.load(f)

    questions = []
    ground_truths = []
    answers = []
    contexts = []

    # 3. Cháº¡y tá»«ng cÃ¢u há»i qua Chatbot
    for item in test_data:
        q = item["question"]
        print(f" -> Äang test: {q}")
        
        # Gá»i Chatbot
        inputs = {"query": q, "chat_history": ""} # Táº¡m thá»i chÆ°a test history
        output = agent_app.invoke(inputs)
        
        # Thu tháº­p káº¿t quáº£
        questions.append(q)
        ground_truths.append(item["ground_truth"])
        answers.append(output.get("generation", "Error"))
        
        # Láº¥y ná»™i dung context (retrieved docs)
        docs = output.get("retrieved_docs", [])
        doc_contents = [d["content"] for d in docs]
        contexts.append(doc_contents)

    # 4. Chuáº©n bá»‹ Dataset cho Ragas
    data_dict = {
        "question": questions,
        "answer": answers,
        "contexts": contexts,
        "ground_truth": ground_truths
    }
    
    from datasets import Dataset
    dataset = Dataset.from_dict(data_dict)

    # 5. Cháº¥m Ä‘iá»ƒm
    print("âš–ï¸  Äang cháº¥m Ä‘iá»ƒm (Ragas)...")
    results = evaluate(
        dataset=dataset,
        metrics=[
            faithfulness,       # Hallucination (0-1)
            answer_relevancy,   # Tráº£ lá»i Ä‘Ãºng Ã½ khÃ´ng (0-1)
            context_precision,  # TÃ¬m Ä‘Ãºng tÃ i liá»‡u khÃ´ng (0-1)
            context_recall,     # TÃ i liá»‡u tÃ¬m Ä‘Æ°á»£c cÃ³ Ä‘á»§ Ã½ so vá»›i Ground Truth khÃ´ng (0-1)
        ],
        llm=evaluator_llm,
        embeddings=evaluator_embeddings
    )

    # 6. Xuáº¥t bÃ¡o cÃ¡o
    print("\nğŸ“Š Káº¾T QUáº¢ ÄÃNH GIÃ:")
    print(results)
    
    df = results.to_pandas()
    df.to_excel("evaluation_report.xlsx", index=False)
    print("âœ… ÄÃ£ lÆ°u bÃ¡o cÃ¡o chi tiáº¿t vÃ o 'evaluation_report.xlsx'")

if __name__ == "__main__":
    run_evaluation()